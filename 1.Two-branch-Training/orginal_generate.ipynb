{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use new network \n",
    "#use the past strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class IntermediateLayerGetter(nn.ModuleDict):\n",
    "    \"\"\"\n",
    "    Module wrapper that returns intermediate layers from a model\n",
    "    It has a strong assumption that the modules have been registered\n",
    "    into the model in the same order as they are used.\n",
    "    This means that one should **not** reuse the same nn.Module\n",
    "    twice in the forward if you want this to work.\n",
    "    Additionally, it is only able to query submodules that are directly\n",
    "    assigned to the model. So if `model` is passed, `model.feature1` can\n",
    "    be returned, but not `model.feature1.layer2`.\n",
    "    Arguments:\n",
    "        model (nn.Module): model on which we will extract the features\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "    Examples::\n",
    "        >>> m = torchvision.models.resnet18(pretrained=True)\n",
    "        >>> # extract layer1 and layer3, giving as names `feat1` and feat2`\n",
    "        >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m,\n",
    "        >>>     {'layer1': 'feat1', 'layer3': 'feat2'})\n",
    "        >>> out = new_m(torch.rand(1, 3, 224, 224))\n",
    "        >>> print([(k, v.shape) for k, v in out.items()])\n",
    "        >>>     [('feat1', torch.Size([1, 64, 56, 56])),\n",
    "        >>>      ('feat2', torch.Size([1, 256, 14, 14]))]\n",
    "    \"\"\"\n",
    "    def __init__(self, model, return_layers):\n",
    "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\n",
    "            raise ValueError(\"return_layers are not present in model\")\n",
    "\n",
    "        orig_return_layers = return_layers\n",
    "        return_layers = {k: v for k, v in return_layers.items()}\n",
    "        layers = OrderedDict()\n",
    "        for name, module in model.named_children():\n",
    "            layers[name] = module\n",
    "            if name in return_layers:\n",
    "                del return_layers[name]\n",
    "            if not return_layers:\n",
    "                break\n",
    "\n",
    "        super(IntermediateLayerGetter, self).__init__(layers)\n",
    "        self.return_layers = orig_return_layers\n",
    "        #self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = OrderedDict()\n",
    "        for name, module in self.named_children():\n",
    "#             print(name)\n",
    "            out[name] = module(x)\n",
    "            x= out[name]\n",
    "#             print(x.shape)\n",
    "            if name == 'ada_maxpool':\n",
    "                x=torch.mean(x,(1))\n",
    "                x=x.view(x.size(0))\n",
    "            if name == 'sigmoid':\n",
    "                x = out['relu']\n",
    "            if name =='layer4':\n",
    "                break\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#from .utils import load_state_dict_from_url\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet2(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=2, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet2, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        self.inplanes2 = 1024\n",
    "        self.dilation2 = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "#         self.avgpool = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer11 = self._make_layer(block, 64, 3,flag=True)\n",
    "        self.layer22 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer33 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.ada_maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer2(block, 64, layers[0])\n",
    "\n",
    "        self.layer2 = self._make_layer2(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer2(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer2(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "#         self.ada_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False,flag=False):\n",
    "        if flag:\n",
    "            self.dilation2=self.dilation\n",
    "            self.inplanes2=self.inplanes\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    def _make_layer2(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation2 = self.dilation2\n",
    "        if dilate:\n",
    "            self.dilation2 *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes2 != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes2, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes2, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation2, norm_layer))\n",
    "        self.inplanes2 = planes * block.expansion\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes2, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "#         self.inplanes2 = self.inplanes\n",
    "#         self.dilation2 = self.dilation\n",
    "        \n",
    "        y = self.maxpool(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         self.inplanes2 = self.inplanes\n",
    "#         self.dilation2 = self.dilation\n",
    "#         print(x.shape)\n",
    "#         print(y.shape)\n",
    "#         print(self.inplanes,self.dilation,self.inplanes2,self.dilation2)\n",
    "#         y = self.layer1(y)\n",
    "        x = self.layer11(x)\n",
    "        y = self.layer1(y)\n",
    "        y = self.layer2(y)\n",
    "        x = self.layer22(x)\n",
    "        y = self.layer3(y)\n",
    "        x = self.layer33(x)\n",
    "        y = self.layer4(y)\n",
    "        x = self.ada_maxpool(x)#所以这里都是均值 \n",
    "        y = self.ada_avgpool(y)\n",
    "        y = y.reshape(y.size(0), -1)\n",
    "        y = self.fc(y)\n",
    "        y = self.softmax(y)\n",
    "        \n",
    "        x = torch.mean(x,(1))\n",
    "        \n",
    "#         print(\"fdkjsfds\")\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0))\n",
    "        x = self.sigmoid(x)\n",
    "# #         print(x.item())\n",
    "# #         print(y.item())\n",
    "# #         x = x*1+y*0\n",
    "# #         print(x.item())\n",
    "# #         print(x.item())\n",
    "# #         print(x.shape)\n",
    "# #         print(y.shape)\n",
    "# #         print(\"fdkjlsf\")\n",
    "        out = OrderedDict()\n",
    "        out['x'] = x\n",
    "        out['y'] = y\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet2(block, layers, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def resnet101_(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 9, 3], pretrained, progress,\n",
    "                   **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import resnet101\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from copy import deepcopy\n",
    "\n",
    "from matplotlib import cm\n",
    "import scipy.misc as misc\n",
    "import matplotlib\n",
    "import imageio\n",
    "\n",
    "def get_jet():\n",
    "    colormap_int = np.zeros((256, 3), np.uint8)\n",
    "    colormap_float = np.zeros((256, 3), np.float)\n",
    "\n",
    "    for i in range(0, 256, 1):\n",
    "        colormap_float[255-i, 0] = cm.jet(i)[0]\n",
    "        colormap_float[255-i, 1] = cm.jet(i)[1]\n",
    "        colormap_float[255-i, 2] = cm.jet(i)[2]\n",
    "\n",
    "        colormap_int[255-i, 0] = np.int_(np.round(cm.jet(i)[0] * 255.0))\n",
    "        colormap_int[255-i, 1] = np.int_(np.round(cm.jet(i)[1] * 255.0))\n",
    "        colormap_int[255-i, 2] = np.int_(np.round(cm.jet(i)[2] * 255.0))\n",
    "        #colormap_int = colormap_int[::-1]\n",
    "    return colormap_int\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ban_tanh(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x=torch.clamp(x,min=0)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "# def load_data(img_path,label_path):\n",
    "#     labels_list=open(label_path, 'r')\n",
    "#     X,Y,Z=[],[],[]\n",
    "#     i=1\n",
    "#     for line in labels_list:\n",
    "#         info=str.split(line,\",\") # info[1]label； info[2]图片文件名\n",
    "#         X.append(np.concatenate([deepcopy (np.expand_dims(imageio.imread(info[1]),axis=2))for i in range(3)],axis=2))\n",
    "#         Y.append(int(info[0]))\n",
    "#         Z.append(str(info[2]))\n",
    "#         i+=1\n",
    "#         if i%100==0:\n",
    "#             print(i)\n",
    "# #         print(X)\n",
    "# #         break;\n",
    "#     return list(zip(X,Y,Z))\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, submodule, extracted_layers):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.submodule = submodule\n",
    "        self.extracted_layers = extracted_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        _name = []\n",
    "        for name, module in self.submodule._modules.items():\n",
    "            if name is \"fc\": x = x.view(x.size(0), -1)\n",
    "            #print(name)\n",
    "            _name.append(name)\n",
    "            x = module(x)\n",
    "\n",
    "            #if name in self.extracted_layers:\n",
    "            outputs.append(x)\n",
    "        return list(zip(_name,outputs))\n",
    "class FeatureExtractor2(nn.Module):\n",
    "    def __init__(self, submodule, extracted_layers):\n",
    "        super(FeatureExtractor2, self).__init__()\n",
    "        self.submodule = submodule\n",
    "        self.extracted_layers = extracted_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for name, module in self.submodule._modules.items():\n",
    "            if name is \"fc\": x = x.view(x.size(0), -1)\n",
    "           # print(name)\n",
    "            x = module(x)\n",
    "\n",
    "            if name in self.extracted_layers:\n",
    "                outputs.append(x)\n",
    "        return outputs\n",
    "    \n",
    "def normalize_255_uchar(array):\n",
    "    Max=np.max(array)\n",
    "    Min=np.min(array)\n",
    "    return (((array-Min)/(Max-Min))*255).astype(np.uint8)\n",
    "learning_rate=0.0001\n",
    "\n",
    "model=resnet101_().cuda()\n",
    "#model=cam_model()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "class_num = 2\n",
    "channel_in = model.fc.in_features\n",
    "model.fc = nn.Linear(channel_in,class_num).cuda()#注意加上这个cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv,os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    }
   ],
   "source": [
    "#you should always put the image at the second place \n",
    "import csv,os\n",
    "def get_csv():\n",
    "    class_path  =  'F:\\\\Ruiveen2\\\\Data\\\\Train_c1\\\\expert1\\\\'\n",
    "    class_path3  = 'F:\\\\Ruiveen2\\\\Data\\\\img\\\\'\n",
    "    blanket=set()\n",
    "    #############case3\n",
    "#     blanket.add(6)\n",
    "#     blanket.add(21)\n",
    "#     blanket.add(27)\n",
    "#################case1\n",
    "    blanket.add(8)\n",
    "    blanket.add(9)\n",
    "    blanket.add(2)\n",
    "###############case2\n",
    "#     blanket.add(5)\n",
    "#     blanket.add(15)\n",
    "#     blanket.add(18)\n",
    "###############case4\n",
    "#     blanket.add(7)\n",
    "#     blanket.add(29)\n",
    "#     blanket.add(34)\n",
    "\n",
    "#     if 2 in blanket:\n",
    "#         print(\"fdsfdsfds\")\n",
    "    with open('my_train.csv','w',newline='')as f:\n",
    "        f_csv = csv.writer(f)\n",
    "        i=0\n",
    "        for img_name in os.listdir(class_path):\n",
    "            info=str.split(img_name,\"_\")\n",
    "            \n",
    "#             print(img_name)\n",
    "#             break\n",
    "           # print(info[0])\n",
    "            if int(info[0]) not in blanket:\n",
    "                class_path2=class_path+img_name+\"\\\\\"\n",
    "#                 print(class_path2)\n",
    "#                 break\n",
    "                for img_name2 in os.listdir(class_path2):\n",
    "                    img_name3=info[1]+\"_\"+img_name2\n",
    "                   # print(img_name3)\n",
    "                    img_path2= class_path2+ img_name2\n",
    "                    img_path = class_path3+info[1]+\"\\\\\"+ img_name2\n",
    "#                     print(img_path)\n",
    "#                     print(img_path2)\n",
    "#                     break\n",
    "                    temp=[2]\n",
    "                    temp.append(img_path)\n",
    "                    temp.append(img_name3)\n",
    "                    temp.append(img_path2)\n",
    "                    temp.append(\"\")\n",
    "                    f_csv.writerow(temp)\n",
    "                    i=i+1\n",
    "                    if i%100==0:\n",
    "                        print(i)\n",
    "#                     break\n",
    "             #   break\n",
    "get_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(label_path):\n",
    "    labels_list=open(label_path, 'r')\n",
    "    X,Y,Z=[],[],[]\n",
    "    i=1\n",
    "    for line in labels_list:\n",
    "        info=str.split(line,\",\") \n",
    "        if i==1:\n",
    "            print(info)\n",
    "        X.append(np.expand_dims(imageio.imread(info[1]),axis=2))\n",
    "        a=imageio.imread(info[3])\n",
    "        b=np.max(a)\n",
    "        if b!=0:\n",
    "            b=1\n",
    "        Y.append(b)\n",
    "        Z.append(str(info[2]))\n",
    "        #break\n",
    "#         print(X[0].shape)\n",
    "#         print(X[0].shape)\n",
    "        i+=1\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "#             if i==500:\n",
    "#                 break\n",
    "    return list(zip(X,Y,Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', 'F:\\\\Ruiveen2\\\\Data\\\\img\\\\17618\\\\1.bmp', '17618_1.bmp', 'F:\\\\Ruiveen2\\\\Data\\\\Train_c1\\\\expert1\\\\15_17618\\\\1.bmp', '\\n']\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    }
   ],
   "source": [
    "data=load_data(\"my_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"com_heat.pkl\"))\n",
    "return_layers = {'layer4': 'layer4'}\n",
    "model=IntermediateLayerGetter(model, return_layers=return_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmpp=87685435454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_G(outputs,list1,_,flag):# get the middle image\n",
    "    for k,v in outputs.items():\n",
    "        temp = np.squeeze(v.cpu().numpy())\n",
    "#         if k=='ada_maxpool':\n",
    "#             temp2=np.max(temp)\n",
    "#             print(temp2)\n",
    "#             list1['layer33_num']=temp2\n",
    "        if k=='sigmoid' or k =='ada_maxpool':\n",
    "            list1[k]=temp\n",
    "            continue\n",
    "        temp=normalize_255_uchar(np.sum(temp,axis=0)).astype(np.int)\n",
    "#         print(temp.shape)\n",
    "        c=temp.shape[0]\n",
    "        d=temp.shape[1]\n",
    "        hp=np.zeros((c,d,3),np.uint8)\n",
    "        for i in range(c):\n",
    "            for j in range(d):\n",
    "                hp[i][j]=color_map[temp[i][j]]\n",
    "#         print(_)\n",
    "        if flag:\n",
    "            cv2.imwrite(\"val_tanh_bce_nofc_c8/\" +_+\"_\"+k+\".bmp\", hp)\n",
    "        list1[k]=temp\n",
    "#         print (k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_gain3(v3,sf2,m,n,o,tp):\n",
    "#     v3=torch.sum(v3,dim=1)\n",
    "#     v3=np.squeeze(n_255(v3).cpu().numpy().astype(np.uint8))\n",
    "    c=v3.shape[0]\n",
    "    d=v3.shape[1]\n",
    "    sf2 = (cv2.resize(sf2, dsize=(d, c)))\n",
    "    ret,sf3=cv2.threshold(sf2,200,255,cv2.THRESH_BINARY)\n",
    "    if np.max(sf3)==0:\n",
    "        ret,sf2=cv2.threshold(sf2,170,255,cv2.THRESH_BINARY)\n",
    "    else :\n",
    "        sf2=sf3\n",
    "    thres_=-9999\n",
    "    for i in range(c):\n",
    "        for j in range(d):\n",
    "            if sf2[i][j]==255:\n",
    "                thres_=max(thres_,v3[i][j])\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    for k in range(m):\n",
    "        sf2 = cv2.dilate(sf2, kernel)\n",
    "        for i in range(c):\n",
    "            for j in range(d):\n",
    "                if v3[i][j]>thres_:\n",
    "                    sf2[i][j]=0\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (o, 5))\n",
    "    if tp==0:\n",
    "        for k in range(n):\n",
    "            sf2 = cv2.dilate(sf2, kernel)\n",
    "            for i in range(c):\n",
    "                for j in range(d):\n",
    "                    if sf2[i][j]>0:\n",
    "                        if v3[i][j]>thres_:\n",
    "                            sf2[i][j]=0\n",
    "    else:\n",
    "        lm=9999999\n",
    "        rm=-9899999\n",
    "        \n",
    "        for k in range(n):\n",
    "            sf2 = cv2.dilate(sf2, kernel)\n",
    "            for i in range(c):\n",
    "                for j in range(d):\n",
    "                    if sf2[i][j]>0:\n",
    "                        if v3[i][j]>thres_:\n",
    "                            sf2[i][j]=0\n",
    "                        else:\n",
    "                            lm=min(lm,j)\n",
    "                            rm=max(rm,j)\n",
    "        sf=np.sum(sf2/255,axis=0)\n",
    "        sf3=np.zeros(128,np.uint8)\n",
    "        for i in range(0,sf.shape[0]-2):\n",
    "            if sf[i]>2:\n",
    "                sf3[i]=abs(sf[i+1]-sf[i])\n",
    "       # print(sf3)\n",
    "        a=np.argmax(sf3)\n",
    "        if sf3[a]>10 and sf[0]>9:\n",
    "            for i in range(c):\n",
    "                for j in range(0,a+3):\n",
    "                    sf2[i][j]=0\n",
    "        else :\n",
    "            for k in range(n):\n",
    "                lt=9999999\n",
    "                rt=-9899999\n",
    "                sf2 = cv2.dilate(sf2, kernel)\n",
    "                for i in range(c):\n",
    "                    for j in range(d):\n",
    "                        if sf2[i][j]>0:\n",
    "                            if v3[i][j]>thres_:\n",
    "                                sf2[i][j]=0\n",
    "                            else:\n",
    "                                lt=min(lt,j)\n",
    "                                rt=max(rt,j)\n",
    "                if lt>=lm or rt<=rm:\n",
    "                    break\n",
    "\n",
    "        print(lm,rm)\n",
    "    return sf2,thres_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanket=set()\n",
    "cck=[]\n",
    "pl=np.zeros(1024).astype(np.int)\n",
    "pr=np.zeros(1024).astype(np.int)\n",
    "def _sum(u,x,y):\n",
    "    t=0\n",
    "#     if x+2>=1024||y+2>=512:\n",
    "#         return 456654564564\n",
    "#     print(x,y)\n",
    "    for i in range(x-1,x+2):\n",
    "        for j in range(y-1,y+2):\n",
    "            t+=u[i][j]\n",
    "    return t/9.0\n",
    "def _change(u,x,y):\n",
    "    for i in range(x-1,x+2):\n",
    "        for j in range(y-1,y+2):\n",
    "            u[i][j]=255\n",
    "def _change2(x,y):\n",
    "    ep=[(x-1,y-1),(x,y-1),(x+1,y-1)]\n",
    "    for i,j in ep:\n",
    "        if pl[i]==0:\n",
    "            pl[i]=j\n",
    "        else:\n",
    "            pl[i]=min(pl[i],j)\n",
    "    ep=[(x-1,y+1),(x,y+1),(x+1,y+1)]\n",
    "    for i,j in ep:\n",
    "        if pr[i]==0:\n",
    "            pr[i]=j\n",
    "        else:\n",
    "            pr[i]=max(pr[i],j)\n",
    "def _gain(g):\n",
    "    for i in range(pl.shape[0]):\n",
    "        if pl[i]!=0 and pr[i]>pl[i]:\n",
    "                col=150\n",
    "                mid=int((pl[i]+pr[i])/2.0)\n",
    "                for j in range(pl[i],mid+1):\n",
    "                    if col<230:\n",
    "                        col+=1\n",
    "                    g[i][j]=col\n",
    "                col=150\n",
    "                for j in range(pr[i],mid,-1):\n",
    "                    if col<230:\n",
    "                        col+=1\n",
    "                    g[i][j]=col\n",
    "def _gain2(g):\n",
    "    for i in range(pl.shape[0]):\n",
    "        if pl[i]!=0 and pr[i]>pl[i]:\n",
    "                for j in range(pl[i],pr[i]):\n",
    "                    g[i][j]=255\n",
    "def _get_f(f,g):\n",
    "    for i in range(f.shape[0]):\n",
    "        for j in range(f.shape[1]):\n",
    "            g[i][j]=f[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_search2(feature_sum,thred,feature_sum3,layer):\n",
    "    c=feature_sum.shape[0]\n",
    "    d=feature_sum.shape[1]\n",
    "    print(c,d)\n",
    "    print(feature_sum3.shape)\n",
    "    feature_sum3 = (cv2.resize(feature_sum3, dsize=(d,c)))\n",
    "    global blanket1\n",
    "    global blanket2\n",
    "    blanket1=set()\n",
    "    blanket2=set()\n",
    "    list1=[]\n",
    "    sf=np.zeros((c,d),np.uint8)\n",
    "   # print(np.max(feature_sum3))\n",
    "    energy=10\n",
    "    e1=np.max(feature_sum3)\n",
    "  #  print(e1)\n",
    "    #if layer==0:\n",
    "    energy=e1-10\n",
    "    for i in range(c):\n",
    "        for j in range(d):\n",
    "            if feature_sum3[i][j]>energy:\n",
    "                blanket1.add(i*c+j);\n",
    "                cc=[(i+4,j),(i-4,j),(i,j-7),(i,j+7)]\n",
    "                for (xi,xj) in cc:\n",
    "                    if xi>=c or xj>=d:\n",
    "                        break\n",
    "                    list1.append(xi*c+xj)\n",
    "            elif feature_sum[i][j]>thred:\n",
    "                sf[i][j]=0\n",
    "            else:\n",
    "                sf[i][j]=255\n",
    "    te=False\n",
    "    heatmap2=np.zeros((c,d,3),np.uint8)\n",
    "    for i in range(c):\n",
    "        for j in range(d):\n",
    "            if feature_sum3[i][j]>energy:\n",
    "                heatmap2[i][j]=color_map[150]\n",
    "            else: \n",
    "                heatmap2[i][j]=color_map[sf[i][j]]\n",
    "    cv2.imwrite(\"val_tanh_bce_nofc_c8/\" +\"fdsfds.bmp\", heatmap2)\n",
    "    for k in blanket1:\n",
    "        if k in blanket2:\n",
    "            continue\n",
    "        if dfs2(sf,k,0,layer)==1:\n",
    "            te=True\n",
    "            break\n",
    "#     print(te)\n",
    "#     print(len(blanket2))\n",
    "    if len(blanket2)==0:\n",
    "        for k in list1:\n",
    "            if k in blanket2:\n",
    "                continue\n",
    "            if dfs2(sf,k,0,layer)==1:\n",
    "                te=True\n",
    "                break\n",
    "    elif len(blanket2)<20:\n",
    "        kernel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "        sq = cv2.dilate(sf, kernel)\n",
    "        blanket2.clear()\n",
    "        for k in list1:\n",
    "            if k in blanket2:\n",
    "                continue\n",
    "            if dfs2(sq,k,0,layer)==1:\n",
    "                te=True\n",
    "                break\n",
    "        if len(blanket2)/20<5 and te==False:\n",
    "            print(len(blanket2)/20)\n",
    "            blanket2.clear()\n",
    "            for k in blanket1:\n",
    "                if k in blanket2:\n",
    "                    continue\n",
    "                if dfs2(sf,k,0,layer)==1:\n",
    "                    te=True\n",
    "                    break\n",
    "    print(len(blanket2))\n",
    "    kkqq=False\n",
    "    if len(blanket2)>999:\n",
    "        print(\"error1\")\n",
    "        kkqq=True\n",
    "        te=False\n",
    "        blanket2.clear()\n",
    "        kernel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "        sf = cv2.erode(sf, kernel)\n",
    "        for k in list1:\n",
    "            if k in blanket2:\n",
    "                continue\n",
    "            if dfs2(sf,k,0,layer)==1:\n",
    "                te=True\n",
    "                break\n",
    "        if te:\n",
    "            blanket2.clear()\n",
    "    print(len(blanket2))\n",
    "    return sf,kkqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global blanket1\n",
    "global blanket2\n",
    "global blanket3\n",
    "blanket1=set()\n",
    "blanket2=set()\n",
    "blanket2=set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs2(sf,k,dep,layer):#sf 搜索图 f填充的特征图,s哈希\n",
    "    c=sf.shape[0]\n",
    "    d=sf.shape[1]\n",
    "    if dep>1000:\n",
    "        return 1\n",
    "    xi=int(k/c)\n",
    "    xj=k%c\n",
    "    e=[]\n",
    "    if layer==2:\n",
    "        if dep!=0:\n",
    "            e=[(xi,xj+1),(xi,xj-1),(xi+1,xj),(xi-1,xj)]\n",
    "        else:\n",
    "            e=[(xi,xj+3),(xi,xj-3),(xi+3,xj),(xi-3,xj)]\n",
    "    elif layer==1 or layer==0:\n",
    "        e=[(xi,xj+1),(xi,xj-1),(xi+1,xj),(xi-1,xj)]\n",
    "    for (i,j) in e:\n",
    "        if i>=c or j>=d:\n",
    "            return \n",
    "        if sf[i][j]==0:\n",
    "            continue\n",
    "        k=i*c+j\n",
    "        if k in blanket2:\n",
    "            continue\n",
    "        blanket2.add(k)\n",
    "        te=dfs2(sf,k,dep+1,layer)\n",
    "        if te==1:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_255(array):\n",
    "    Max=torch.max(array)\n",
    "    Min=torch.min(array)\n",
    "    return (((array-Min)/(Max-Min))*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "-----------------------------------\n",
      "0\n",
      "0\n",
      "0.5002168\n",
      "-----------------------------------\n",
      "2\n",
      "0\n",
      "0.5002257\n",
      "-----------------------------------\n",
      "4\n",
      "1\n",
      "0.99995863\n",
      "6522_100.bmp\n",
      "128 64\n",
      "(128, 64)\n",
      "151\n",
      "151\n",
      "BBBBB\n",
      "1 73\n",
      "\n",
      "-----------------------------------\n",
      "6\n",
      "1\n",
      "0.9999796\n",
      "6522_101.bmp\n",
      "128 64\n",
      "(128, 64)\n",
      "145\n",
      "145\n",
      "BBBBB\n",
      "1 77\n",
      "\n",
      "-----------------------------------\n",
      "8\n",
      "1\n",
      "0.99997926\n",
      "6522_102.bmp\n",
      "128 64\n",
      "(128, 64)\n",
      "145\n",
      "145\n",
      "BBBBB\n",
      "1 73\n",
      "\n",
      "-----------------------------------\n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-56d6fa72159b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mlist_m\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mcnt\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mget_G\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_m\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mtar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist_m\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-58a56a77f100>\u001b[0m in \u001b[0;36mget_G\u001b[1;34m(outputs, list1, _, flag)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0mhp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m#         print(_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#test set\n",
    "t_lay=[]   #get the middle image\n",
    "t_r=[]\n",
    "import matplotlib.pyplot as plt # 比较完整的生成膨胀后的图\n",
    "import copy\n",
    "with torch.no_grad():\n",
    "    global blanket1\n",
    "    global blanket2\n",
    "    k = 0\n",
    "    #fft=False\n",
    "    print(len(data))\n",
    "    cnt=0\n",
    "    for img, tar,_ in data:\n",
    "#         if _!=\"15382_100.bmp\":\n",
    "#             continue\n",
    "        print(\"-----------------------------------\")\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "#         if fft==False :\n",
    "#         if _ !=\"9757_39.bmp\":\n",
    "#             continue\n",
    "#             else :\n",
    "#                 fft=True\n",
    "#         if _!=\"120.1.bmp\":\n",
    "#             continue\n",
    "#         if _!=\"17618_120.bmp\":\n",
    "#             continue\n",
    "        tk=img\n",
    "        img=np.concatenate([deepcopy (np.expand_dims(img,axis=2))for i in range(3)],axis=2)#.transpose(3,2, 0, 1)\n",
    "        img = np.squeeze(img)#.transpose((1, 2, 0))\n",
    "#             print(img.shape)\n",
    "#             break\n",
    "        #print(te.shape)\n",
    "        tk = np.expand_dims(tk.transpose((2, 0, 1)), 0)\n",
    "        #print(tk.shape)\n",
    "       # img = torch.from_numpy(img).cuda().float()\n",
    "        tk = torch.from_numpy(tk).cuda().float()\n",
    "#         print(soft_max)\n",
    "#         print(label)\n",
    "#         print(_)\n",
    "        color_map=get_jet()\n",
    "      #  data2=layers_features(img)\n",
    "        outputs=model(tk)\n",
    "        list_m={}\n",
    "        cnt+=1\n",
    "        get_G(outputs,list_m,_,0)\n",
    "        print(tar)\n",
    "        tar=list_m['sigmoid']\n",
    "        print(tar)\n",
    "        if tar<0.9:\n",
    "#             continue\n",
    "            sf2=np.zeros((1024,512),np.uint8)\n",
    "            cv2.imwrite(\"val_tanh_bce_nofc_c8/\"+\"_\" +_, sf2)\n",
    "            continue\n",
    "        else:\n",
    "#             continue\n",
    "            feature_sum3=list_m['layer33']#normalize_255_uchar(np.sum(layer3_feature,axis=0)).astype(np.int)#有种子点\n",
    "            feature_sum=list_m['layer2']\n",
    "            e=np.mean(feature_sum3)\n",
    "            print(_)\n",
    "            c=feature_sum.shape[0]\n",
    "            d=feature_sum.shape[1]\n",
    "            heatmap=np.zeros((c,d,3),np.uint8)\n",
    "            sf2=np.zeros((c,d),np.uint8)\n",
    "#             img = np.squeeze(img.data.cpu().numpy()).transpose((1, 2, 0))\n",
    "            tk = np.squeeze(tk.data.cpu().numpy())\n",
    "#             listt1={}\n",
    "#             listt2={}\n",
    "#             get_G(outputs2,outputs,listt1,listt2)\n",
    "            \n",
    "#             for k,v in listt1.items():\n",
    "#                 print (k,v.shape)\n",
    "#             print(\"outkdfkjslkjdfslkj2\")\n",
    "#             for k,v in outputs.items():\n",
    "#                 temp = np.squeeze(v.cpu().numpy())\n",
    "#                 temp=normalize_255_uchar(np.sum(temp,axis=0)).astype(np.int)\n",
    "#                 c=temp.shape[0]\n",
    "#                 d=temp.shape[1]\n",
    "#                 hp=np.zeros((c,d,3),np.uint8)\n",
    "#                 for i in range(c):\n",
    "#                     for j in range(d):\n",
    "#                         hp[i][j]=color_map[temp[i][j]]\n",
    "#                 cv2.imwrite(\"val_tanh_bce_nofc_c8/\" +\"cnn_\"+k+\".bmp\", hp)\n",
    "#                 print (k,v.shape)\n",
    "#             print(\"outkdfkjslkjdfslkj\")\n",
    "#                 break\n",
    "\n",
    "            if 0 :\n",
    "                ;\n",
    "\n",
    "            else:\n",
    "                g=np.zeros((c,d),np.uint8)\n",
    "                _get_f(feature_sum3,g)\n",
    "#                 e1=np.median(feature_sum)\n",
    "                e1=np.max(feature_sum, axis=1)\n",
    "            #     print(temp2.shape)\n",
    "            #     print(temp2)\n",
    "#                 e1=np.min(e1)\n",
    "                e1.sort()\n",
    "                e1=e1[5]\n",
    "#                 e1*=2\n",
    "                sf,kkqq=feature_search2(feature_sum,e1,g,2)\n",
    "                tp=\"\"\n",
    "                if len(blanket2)<8 and kkqq==False:\n",
    "                    layer4_feature = np.squeeze(outputs['layer1'].cpu().numpy())\n",
    "                    feature_sum=normalize_255_uchar(np.sum(layer4_feature,axis=0)).astype(np.int)\n",
    "                    c=feature_sum.shape[0]\n",
    "                    d=feature_sum.shape[1]\n",
    "                    heatmap=np.zeros((c,d,3),np.uint8)\n",
    "#                     e1=np.median(feature_sum)\n",
    "#                     e1*=2\n",
    "                    e1=np.max(feature_sum, axis=1)\n",
    "#                     e1=np.min(e1)\n",
    "                    e1.sort()\n",
    "                    e1=e1[5]\n",
    "                    sf,kkqq=feature_search2(feature_sum,e1,g,1)\n",
    "                    sf2=np.zeros((c,d),np.uint8)\n",
    "                    tp=\"001.\"\n",
    "                if len(blanket2)<8 and kkqq==False:\n",
    "                    layer4_feature = np.squeeze(outputs['relu'].cpu().numpy())\n",
    "                    feature_sum=normalize_255_uchar(np.sum(layer4_feature,axis=0)).astype(np.int)\n",
    "                    c=feature_sum.shape[0]\n",
    "                    d=feature_sum.shape[1]\n",
    "                    heatmap=np.zeros((c,d,3),np.uint8)\n",
    "                    e1=np.max(feature_sum, axis=1)\n",
    "                    e1.sort()\n",
    "                    e1=e1[5]\n",
    "                    sf,kkqq=feature_search2(feature_sum,e1,g,0)\n",
    "                    sf2=np.zeros((c,d),np.uint8)\n",
    "                    tp=\"000.\"\n",
    "                if len(blanket2)<8 and kkqq==False:\n",
    "                    continue\n",
    "                    print(\"AAAAA\")\n",
    "                    tp=\"A\"\n",
    "                    blanket.clear()\n",
    "                    cck=[]\n",
    "                    pl=np.zeros(1024).astype(np.int)\n",
    "                    pr=np.zeros(1024).astype(np.int)\n",
    "                    f=img.shape[0]/feature_sum3.shape[0]\n",
    "                    c=feature_sum3.shape[0]\n",
    "                    d=feature_sum3.shape[1]\n",
    "                   # sf2=np.zeros((c,d),np.uint8)\n",
    "                    g=np.zeros((c,d),np.uint8)\n",
    "                    _get_f(feature_sum3,g)\n",
    "                    c=img.shape[0]\n",
    "                    d=img.shape[1]\n",
    "                    g=cv2.resize(g, dsize=(d,c))\n",
    "                    #maxi,maxj= np.where(g==np.max(g))\n",
    "                    #print(maxi,maxj)\n",
    "                    h = tk\n",
    "                    #for (mi,mj) in zip(maxi,maxj):\n",
    "                    for i in range(c):\n",
    "                        for j in range(d):\n",
    "                            if g[i][j]>30:\n",
    "                                w=ppq(h,g,str(int(i))+str('+')+str(int(j)),0)\n",
    "    #                 sf2=np.zeros((c,d),np.uint8)\n",
    "                    _gain2(g)\n",
    "                    c=feature_sum.shape[0]\n",
    "                    d=feature_sum.shape[1]\n",
    "                    #cv2.imwrite(\"val_tanh_bce_nofc_c2/\" +\"ooooods.bmp\", g)\n",
    "                    g=cv2.resize(g, dsize=(d,c))\n",
    "                    cv2.imwrite(\"val_tanh_bce_nofc_c8/\" +\"ooooods.bmp\", g)\n",
    "                    sf2=np.zeros((c,d),np.uint8)\n",
    "                    print(sf2.shape)\n",
    "                    print(g.shape)\n",
    "                    print(feature_sum3.shape)\n",
    "                    for i in range(c):\n",
    "                        for j in range(d):\n",
    "                          #  print(sf2[i][j])]\n",
    "                            if g[i][j]>200:\n",
    "                                sf2[i][j]=255\n",
    "                            elif g[i][j]>100:\n",
    "                                sf2[i][j]=255\n",
    "                    c=tk.shape[0]\n",
    "                    d=tk.shape[1]\n",
    "                    sf3 = (cv2.resize(sf2, dsize=(d,c)))\n",
    "                    ret,sf3=cv2.threshold(sf3,200,255,cv2.THRESH_BINARY)\n",
    "                    cv2.imwrite(\"val_tanh_bce_nofc_c8/\"+\"_\" +_, sf3)\n",
    "#                     break\n",
    "                  #  heatmap=np.zeros((c,d,3),np.uint8)\n",
    "                  #  continue\n",
    "                elif kkqq==False :\n",
    "                    #continue\n",
    "                    print(\"BBBBB\")\n",
    "    #                 c=feature_sum.shape[0]\n",
    "    #                 d=feature_sum.shape[1]\n",
    "    #                 g=cv2.resize(g, dsize=(d,c))\n",
    "                    for k in blanket2:\n",
    "                        i=int(k/c)\n",
    "                        j=k%c\n",
    "                        if i>=c or j>=d:\n",
    "                            continue\n",
    "                        sf2[i][j]=255\n",
    "                   # print(np.max(sf2))\n",
    "    #                 for k,v in outputs.items():\n",
    "    #                     print(k)\n",
    "    #                     print(v.shape)\n",
    "#                     print(\"--0099--\")\n",
    "#                     print(sf2.shape)\n",
    "#                     get_G2(sf2,\"layer2\",\"merge_\")\n",
    "#                     get_G3(sf2,listt1['layer2'],\"layer2\",\"show_\")\n",
    "                    \n",
    "#                     print(\"layer2\")\n",
    "                    if tp==\"\":\n",
    "                        v3=outputs['layer1']\n",
    "                        v3=torch.sum(v3,dim=1)\n",
    "                        v3=np.squeeze(n_255(v3).cpu().numpy().astype(np.uint8))\n",
    "                        if len(blanket2)<20:\n",
    "                            sf2,res=layer_gain3(v3,sf2,2,8,8,0)\n",
    "                        else :\n",
    "                            sf2,res=layer_gain3(v3,sf2,2,4,5,1)\n",
    "#                     print(\"--0099--\")\n",
    "#                     get_G2(sf2,\"layer1\",\"merge_\")\n",
    "#                     get_G3(sf2,listt1['layer1'],\"layer1\",\"show_\")\n",
    "#                     print(\"layer1\")\n",
    "                  #  print(np.max(sf2))\n",
    "    #                 print(sf2.shape,v3.shape)\n",
    "    #                 c=v3.shape[0]\n",
    "    #                 d=v3.shape[1]\n",
    "    #                 heatmap=np.zeros((c,d,3),np.uint8)\n",
    "    #                 for i in range(c):\n",
    "    #                     for j in range(d):\n",
    "    #                         if sf2[i][j]==255:\n",
    "    #                             heatmap[i][j]=color_map[sf2[i][j]]\n",
    "    #                         else :\n",
    "    #                             heatmap[i][j]=color_map[(v3[i][j])]\n",
    "    #                 cv2.imwrite(\"val_tanh_bce_nofc_c2/\" +\"B\"+_, heatmap)\n",
    "    #                 break\n",
    "                    t_lay.append(res)\n",
    "\n",
    "                    v3=outputs['relu']\n",
    "                    t_r.append(res)\n",
    "                    v3=torch.sum(v3,dim=1)\n",
    "                    v3=np.squeeze(n_255(v3).cpu().numpy().astype(np.uint8))\n",
    "                    if len(blanket2)<20:\n",
    "                        sf2,res=layer_gain3(v3,sf2,2,2,10,0)\n",
    "                    else :\n",
    "                        sf2,res=layer_gain3(v3,sf2,2,1,5,0)\n",
    "#                     get_G2(sf2,\"relu\",\"merge_\")\n",
    "#                     get_G3(sf2,listt1['relu'],\"relu\",\"show_\")\n",
    "#                     print(\"relu\")\n",
    "                    if len(blanket2)<20:\n",
    "                        sf2,res=layer_gain3(tk,sf2,0,2,5,0)\n",
    "                    else :\n",
    "                        sf2,res=layer_gain3(tk,sf2,0,2,5,0)\n",
    "#                     print(sf2.shape,listt1['relu'].shape)\n",
    "#                     get_G2(sf2,\"relu\",\"merge_\")\n",
    "#                     get_G3(sf2,listt1['relu'],\"relu\",\"show_\")\n",
    "                    print()\n",
    "                    c=tk.shape[0]\n",
    "                    d=tk.shape[1]\n",
    "                    sf3 = (cv2.resize(sf2, dsize=(d,c)))\n",
    "                    ret,sf3=cv2.threshold(sf3,200,255,cv2.THRESH_BINARY)\n",
    "                    cv2.imwrite(\"val_tanh_bce_nofc_c8/\"+\"_\" +_, sf2)\n",
    "                    sf2 = (cv2.resize(sf2, dsize=(d,c))*0.5).astype(np.int)\n",
    "                    heatmap=np.zeros((c,d,3),np.uint8)\n",
    "                    for i in range(c):\n",
    "                        for j in range(d):\n",
    "                            heatmap[i][j]=color_map[sf2[i][j]]\n",
    "\n",
    "                    show_img = np.clip(heatmap + img, a_min=0, a_max=255).astype(np.uint8)\n",
    "                    img = img.astype(np.uint8)\n",
    "                    write_img = np.concatenate((img, show_img), axis=1)\n",
    "                    write_img = cv2.putText(write_img, \"target:\" + str(tar) , (10, 60), 2,cv2.FONT_HERSHEY_PLAIN, (0, 0, 255))\n",
    "                    cv2.imwrite(\"val_tanh_bce_nofc_c8/\" +_, write_img)\n",
    "#                     break\n",
    "                    continue\n",
    "                else:\n",
    "#                     sf2=np.zeros((1024,512),np.uint8)\n",
    "#                     cv2.imwrite(\"val_tanh_bce_nofc_c8/\"+\"_\" +_, sf2)\n",
    "                    continue\n",
    "                    tp+=\"C\"\n",
    "                    print(\"CCCCC\")\n",
    "                    for i in range(c):\n",
    "                        for j in range(d):\n",
    "                          #  print(sf2[i][j])]\n",
    "                            if feature_sum3[i][j]>10:\n",
    "                                sf2[i][j]=max(feature_sum3[i][j],255)\n",
    "#                     c=tk.shape[0]\n",
    "#                     d=tk.shape[1]\n",
    "#                     sf3 = (cv2.resize(sf2, dsize=(d,c)))\n",
    "#                     ret,sf3=cv2.threshold(sf3,200,255,cv2.THRESH_BINARY)\n",
    "#                     print(sf3.shape)\n",
    "#                     print(sf3)\n",
    "#                     print(_)\n",
    "                   # cv2.imwrite(\"val_tanh_bce_nofc_c8/\"+\"_\" +_, sf3)\n",
    "                  #  continue\n",
    "            c=feature_sum.shape[0]\n",
    "            d=feature_sum.shape[1]\n",
    "            for i in range(c):\n",
    "                    for j in range(d):\n",
    "                        heatmap[i][j]=color_map[sf2[i][j]]\n",
    "            heatmap = (cv2.resize(heatmap, dsize=(img.shape[1], img.shape[0]))*0.5).astype(np.int)\n",
    "            show_img = np.clip(img + heatmap, a_min=0, a_max=255).astype(np.uint8)\n",
    "            img = img.astype(np.uint8)\n",
    "            write_img = np.concatenate((img, show_img), axis=1)\n",
    "            write_img = cv2.putText(write_img, \"target:\" + str(tar) , (10, 60), 2,cv2.FONT_HERSHEY_PLAIN, (0, 0, 255))\n",
    "            cv2.imwrite(\"val_tanh_bce_nofc_c8/\" +tp+_, write_img)\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _9757_36.bmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     try:\n",
    "#         a=imageio.imread(\"_9757_36.bmp\")\n",
    "#     except FileNotFoundError:\n",
    "#         a=np.zeros((1024,512),np.uint8)\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
